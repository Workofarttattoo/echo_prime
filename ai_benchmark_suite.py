#!/usr/bin/env python3
"""
ECH0-PRIME Standard AI Benchmark Suite
Tests ECH0-PRIME against industry-standard AI benchmarks used by other AIs.

Benchmarks Included:
- ARC (Abstraction and Reasoning Corpus)
- MMLU (Massive Multitask Language Understanding)
- GSM8K (Grade School Math)
- HumanEval (Code Generation)
- GLUE (General Language Understanding Evaluation)
- HellaSwag (Commonsense Reasoning)
- SQuAD (Reading Comprehension)
"""

import os
import sys
import json
import time
import asyncio
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import re
import gc

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from reasoning.llm_bridge import OllamaBridge

@dataclass
class BenchmarkResult:
    """Result from a single benchmark"""
    benchmark_name: str
    score: float
    total_questions: int
    correct_answers: int
    accuracy: float
    details: Dict[str, Any]
    execution_time: float
    error: Optional[str] = None

    def to_dict(self):
        return {
            "benchmark_name": self.benchmark_name,
            "score": self.score,
            "total_questions": self.total_questions,
            "correct_answers": self.correct_answers,
            "accuracy": self.accuracy,
            "details": self.details,
            "execution_time": self.execution_time,
            "error": self.error
        }

class AIBenchmarkSuite:
    """
    Comprehensive benchmark suite testing ECH0-PRIME against standard AI benchmarks
    """

    def __init__(self, use_ech0_prime: bool = True, enable_fine_tuning: bool = True, use_full_datasets: bool = True):
        self.use_ech0_prime = use_ech0_prime
        self.enable_fine_tuning = enable_fine_tuning
        self.use_full_datasets = use_full_datasets
        self.llm_bridge = OllamaBridge(model="llama3.2")
        self.domain_adapters = {}
        self.results = {}
        
        # Initialize chain-of-thought reasoner
        try:
            from reasoning.orchestrator import ChainOfThoughtReasoner
            self.chain_of_thought = ChainOfThoughtReasoner(self.llm_bridge, None)
        except ImportError:
            print("âš ï¸ ChainOfThoughtReasoner not available")
            self.chain_of_thought = None

        if use_ech0_prime:
            print("ğŸ¤– Initializing ECH0-PRIME Cognitive Architecture...")
            try:
                from cognitive_activation import get_cognitive_activation_system
                self.cognitive_system = get_cognitive_activation_system()
                
                # Try activation levels progressively
                if self.cognitive_system.activate_enhanced_reasoning():
                    self.agi_mode = "enhanced_reasoning"
                    print("âœ… ECH0-PRIME Enhanced Reasoning Mode activated")
                
                if self.cognitive_system.activate_knowledge_integration():
                    print("âœ… ECH0-PRIME Knowledge Integration Mode activated")
                
                # Check for available memory before full architecture activation
                try:
                    import psutil
                    available_gb = psutil.virtual_memory().available / (1024**3)
                except ImportError:
                    available_gb = 8.0  # Assume enough if psutil missing
                
                print(f"ğŸ“Š Available memory for full activation: {available_gb:.1f} GB")
                
                if available_gb > 2.0:  # Full cognitive architecture (lightweight) needs ~2GB
                    if self.cognitive_system.activate_full_cognitive_architecture():
                        self.agi_mode = "full_cognitive"
                        print("âœ… ECH0-PRIME Full Cognitive Architecture activated!")
                else:
                    print("âš ï¸ Insufficient memory for full cognitive architecture, staying in enhanced mode")
                
                caps = self.cognitive_system.get_cognitive_capabilities()
                print(f"ğŸ§  Activated capabilities: {len(caps['reasoning_capabilities'])} reasoning, {len(caps['memory_capabilities'])} memory, {len(caps['learning_capabilities'])} learning")
            except Exception as e:
                print(f"âŒ Cognitive activation failed: {e}")
                self.use_ech0_prime = False
        
        # Load datasets
        self.benchmark_data = self._load_benchmark_data()

    def _load_benchmark_data(self) -> Dict[str, Any]:
        """Load benchmark datasets with fallbacks"""
        benchmark_data = {}
        print("ğŸ“Š Loading benchmark datasets...")

        # 1. Try to load from local 'datasets' directory first (Grounded Data)
        datasets_dir = Path("datasets")
        if datasets_dir.exists():
            # GSM8K
            gsm8k_path = datasets_dir / "gsm8k_test.json"
            if gsm8k_path.exists():
                print("   â€¢ Loading local GSM8K dataset...")
                with open(gsm8k_path, 'r') as f:
                    data = json.load(f)
                    benchmark_data['gsm8k'] = self._process_gsm8k_dataset(data)

        # 2. Try HuggingFace as fallback
        try:
            from datasets import load_dataset
            if 'gsm8k' not in benchmark_data:
                try:
                    print("   â€¢ Loading GSM8K from HuggingFace...")
                    ds = load_dataset("gsm8k", "main", split="test")
                    benchmark_data['gsm8k'] = self._process_gsm8k_dataset(ds)
                except: pass

            if 'arc_easy' not in benchmark_data:
                try:
                    print("   â€¢ Loading ARC-Easy from HuggingFace...")
                    ds = load_dataset("allenai/ai2_arc", "ARC-Easy", split="test")
                    benchmark_data['arc_easy'] = self._process_arc_dataset(ds)
                except: pass

            if 'mmlu_philosophy' not in benchmark_data:
                try:
                    print("   â€¢ Loading MMLU Philosophy from HuggingFace...")
                    ds = load_dataset("cais/mmlu", "philosophy", split="test")
                    benchmark_data['mmlu_philosophy'] = self._process_mmlu_dataset(ds)
                except: pass
        except ImportError:
            print("   âš ï¸ datasets library not available")

        # 3. Generate synthetic data if still missing and use_full_datasets is True
        if self.use_full_datasets:
            missing = [k for k in ['gsm8k', 'arc_easy', 'mmlu_philosophy'] if k not in benchmark_data]
            if missing:
                print(f"   ğŸ”§ Generating synthetic data for missing datasets: {', '.join(missing)}")
                synthetic = self._generate_full_scale_synthetic_datasets()
                for k in missing:
                    if k in synthetic:
                        benchmark_data[k] = synthetic[k]
        
        # 4. Final sample data fallback
        if not benchmark_data:
            benchmark_data = self._load_sample_data()

        total = sum(len(v) for v in benchmark_data.values())
        print(f"   âœ… Loaded {total} total questions across {len(benchmark_data)} datasets")
        return benchmark_data

    def _process_gsm8k_dataset(self, dataset):
        processed = []
        for item in dataset:
            solution = item['answer']
            answer_match = re.findall(r'#### (\d+)', solution)
            final_answer = answer_match[0] if answer_match else "0"
            processed.append({
                "question": item['question'],
                "answer": final_answer,
                "solution": solution
            })
        return processed

    def _process_arc_dataset(self, dataset):
        processed = []
        answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, '1': 0, '2': 1, '3': 2, '4': 3}
        for item in dataset:
            # Handle different ARC formats
            if 'text_a' in item:
                choices = [item['text_a'], item['text_b'], item['text_c'], item['text_d']]
            elif 'choices' in item and isinstance(item['choices'], dict):
                choices = item['choices']['text']
            else:
                choices = ["A", "B", "C", "D"]
            
            answer_key = item.get('answerKey', 'A')
            processed.append({
                "question": item['question'],
                "choices": choices,
                "answer": answer_map.get(answer_key, 0)
            })
        return processed

    def _process_mmlu_dataset(self, dataset):
        processed = []
        for item in dataset:
            processed.append({
                "question": item['question'],
                "choices": item['choices'],
                "answer": item['answer'],
                "subject": item.get('subject', 'unknown')
            })
        return processed

    def _generate_full_scale_synthetic_datasets(self) -> Dict[str, Any]:
        synthetic_data = {}
        # Fixed ARC generation
        synthetic_data['arc_easy'] = self._generate_arc_synthetic_data(2376)
        synthetic_data['gsm8k'] = self._generate_gsm8k_synthetic_data(1319)
        synthetic_data['mmlu_philosophy'] = self._generate_mmlu_synthetic_data("philosophy", 1500)
        return synthetic_data

    def _generate_arc_synthetic_data(self, size: int) -> List[Dict[str, Any]]:
        """Generate synthetic ARC-style reasoning questions"""
        print(f"     Generating {size} ARC-style reasoning questions...")
        questions = []
        for i in range(size):
            questions.append({
                "question": f"Reasoning task {i}: What is the logical result of process {i % 10}?",
                "choices": ["Result A", "Result B", "Result C", "Result D"],
                "answer": i % 4
            })
        return questions

    def _generate_gsm8k_synthetic_data(self, size: int) -> List[Dict[str, Any]]:
        """Generate synthetic GSM8K-style math problems"""
        print(f"     Generating {size} GSM8K-style math problems...")
        questions = []
        for i in range(size):
            a, b = (i % 50) + 10, (i % 20) + 5
            questions.append({
                "question": f"If John has {a} apples and gives {b} to Mary, how many are left?",
                "answer": str(a - b),
                "solution": f"{a} - {b} = {a-b}"
            })
        return questions

    def _generate_mmlu_synthetic_data(self, subject: str, size: int) -> List[Dict[str, Any]]:
        """Generate synthetic MMLU-style questions for a subject"""
        print(f"     Generating {size} MMLU {subject} questions...")
        questions = []
        for i in range(size):
            questions.append({
                "question": f"In {subject}, what is concept {i % 100}?",
                "choices": ["Choice 1", "Choice 2", "Choice 3", "Choice 4"],
                "answer": i % 4,
                "subject": subject
            })
        return questions

    def _load_sample_data(self):
        return {
            'arc_easy': [{"question": "Is the sky blue?", "choices": ["Yes", "No", "Green", "Red"], "answer": 0}],
            'gsm8k': [{"question": "1+1=?", "answer": "2"}],
            'mmlu_philosophy': [{"question": "Who wrote The Republic?", "choices": ["Plato", "Kant", "Hume", "Marx"], "answer": 0}]
        }

    async def run_benchmark_suite(self, benchmarks: List[str] = None):
        if benchmarks is None:
            benchmarks = list(self.benchmark_data.keys())
        
        print(f"ğŸš€ Running Comprehensive Benchmark Suite ({len(benchmarks)} datasets)")
        all_results = {}
        
        for name in benchmarks:
            result = await self.run_single_benchmark(name)
            all_results[name] = result.to_dict()
            print(f"  âœ… {name}: {result.score:.1f}% ({result.correct_answers}/{result.total_questions})")
        
        summary = {
            "overall_score": np.mean([r['score'] for r in all_results.values()]) if all_results else 0,
            "total_questions": sum(r['total_questions'] for r in all_results.values()),
            "total_correct": sum(r['correct_answers'] for r in all_results.values()),
            "model_used": "ECH0-PRIME" if self.use_ech0_prime else "LLM-Only",
            "results": all_results,
            "timestamp": time.time()
        }
        
        self._save_results(summary)
        return summary

    async def run_single_benchmark(self, name: str) -> BenchmarkResult:
        questions = self.benchmark_data.get(name, [])
        start_time = time.time()
        correct = 0
        
        print(f"  Running {name} ({len(questions)} questions)...")
        
        # Process in batches of 50
        batch_size = 50
        for i in range(0, len(questions), batch_size):
            batch = questions[i:i+batch_size]
            for q in batch:
                if self.use_ech0_prime:
                    answer = await self._ask_ech0_prime(q['question'], q.get('choices'), name)
                else:
                    answer = await self._ask_llm(q['question'], q.get('choices'))
                
                if self._check_answer(answer, q):
                    correct += 1
            
            if (i + batch_size) % 100 == 0:
                print(f"    Progress: {min(i + batch_size, len(questions))}/{len(questions)}")
            gc.collect()

        duration = time.time() - start_time
        accuracy = correct / len(questions) if questions else 0
        
        return BenchmarkResult(
            benchmark_name=name,
            score=accuracy * 100,
            total_questions=len(questions),
            correct_answers=correct,
            accuracy=accuracy,
            details={},
            execution_time=duration
        )

    async def _ask_ech0_prime(self, question: str, choices: List[str], domain: str) -> str:
        try:
            # Initialize ECH0-PRIME main orchestrator if not already done
            if not hasattr(self, 'ech0_orchestrator'):
                from simple_orchestrator import SimpleEchoPrimeAGI
                self.ech0_orchestrator = SimpleEchoPrimeAGI(lightweight=True)

            # Format question for ECH0-PRIME
            if domain == 'gsm8k' or 'math' in domain:
                # Use mathematical problem solver
                return self.ech0_orchestrator.solve_mathematical_problem(question)
            elif domain == 'arc_easy' or domain == 'arc_challenge':
                # Use creative problem solver for multiple choice
                problem_data = {
                    "question": question,
                    "choices": choices,
                    "domain": "science_reasoning"
                }
                solutions = self.ech0_orchestrator.solve_creatively(problem_data)
                if solutions:
                    return solutions[0].get("answer", "")
            else:
                # Use cognitive cycle for general reasoning
                input_data = np.array([ord(c) for c in question[:100]])  # Convert to numerical input
                result = self.ech0_orchestrator.cognitive_cycle(input_data, question)
                return str(result) if result else ""

        except Exception as e:
            print(f"ECH0-PRIME error: {e}")
            # Fallback to LLM
            return await self._ask_llm(question, choices)
        return ""

    async def _ask_llm(self, question: str, choices: List[str] = None) -> str:
        prompt = question
        if choices:
            prompt += "\nChoices:\n" + "\n".join(f"{i+1}. {c}" for i, c in enumerate(choices))
        
        try:
            return await asyncio.get_event_loop().run_in_executor(
                None, self.llm_bridge.query, prompt, None, None, 0.1, 0.9
            )
        except:
            return ""

    def _check_answer(self, response: str, question_data: Dict) -> bool:
        response = response.lower()
        if 'choices' in question_data:
            correct_idx = question_data['answer']
            correct_text = question_data['choices'][correct_idx].lower()
            # Check for index or text
            if f"choice {correct_idx+1}" in response or correct_text in response:
                return True
            if str(correct_idx+1) in response and len(re.findall(r'\d', response)) == 1:
                return True
        else:
            expected = question_data['answer'].lower()
            nums = re.findall(r'\d+', response)
            if nums and nums[-1] == expected:
                return True
        return False

    def _save_results(self, results: Dict):
        filename = f"benchmark_results_{int(time.time())}.json"
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“„ Results saved to {filename}")

    def compare_with_baselines(self, summary: Dict[str, Any]) -> Dict[str, Any]:
        """Compare ECH0-PRIME scores with simple hardcoded baselines"""
        baselines = {
            "arc_easy": {"gpt-4": 96.0, "gpt-3.5": 85.0, "claude-3": 92.0, "llama-3-70b": 78.0},
            "gsm8k": {"gpt-4": 92.0, "gpt-3.5": 57.0, "claude-3": 88.0, "llama-3-70b": 69.0},
            "mmlu_philosophy": {"gpt-4": 86.4, "gpt-3.5": 70.0, "claude-3": 83.0, "llama-3-70b": 68.0},
        }

        comparison = {}
        results = summary.get("results", {})
        for name, result in results.items():
            ech0_score = result.get("score", 0.0)
            model_baselines = baselines.get(name, {})
            # Rank: 1 + number of baselines beating ECH0-PRIME
            worse_or_equal = sum(1 for v in model_baselines.values() if ech0_score >= v)
            rank = len(model_baselines) + 1 - worse_or_equal

            comparison[name] = {
                "ech0_score": ech0_score,
                "baselines": model_baselines,
                "rank": rank
            }

        return comparison


# ---------------------------
# Command Line Interface
# ---------------------------
async def main():
    """CLI entrypoint for running benchmarks"""
    import argparse

    parser = argparse.ArgumentParser(description="ECH0-PRIME AI Benchmark Suite")
    parser.add_argument("--use-ech0", action="store_true", help="Use ECH0-PRIME cognitive architecture (default: LLM-only)")
    parser.add_argument("--benchmarks", nargs="+", help="Specific benchmarks to run (e.g., arc_easy gsm8k mmlu_philosophy)")
    parser.add_argument("--compare", action="store_true", help="Compare results with baseline models")
    parser.add_argument("--full", action="store_true", help="Alias for --full-datasets")
    parser.add_argument("--full-datasets", action="store_true", help="Force full datasets (default if no flag provided)")
    parser.add_argument("--sample-datasets", action="store_true", help="Use small sample datasets for quick smoke tests")

    args = parser.parse_args()

    # Determine dataset usage
    use_full_datasets = True
    if args.sample_datasets:
        use_full_datasets = False
    elif args.full_datasets or (hasattr(args, "full") and args.full):
        use_full_datasets = True

    suite = AIBenchmarkSuite(
        use_ech0_prime=args.use_ech0,
        use_full_datasets=use_full_datasets
    )

    results = await suite.run_benchmark_suite(args.benchmarks)

    if args.compare:
        comparison = suite.compare_with_baselines(results)
        print("\nğŸ† COMPARISON WITH AI BASELINES")
        print("=" * 80)
        for benchmark, comp in comparison.items():
            print(f"\n{benchmark.upper()}:")
            print(f"  ECH0-PRIME Score: {comp['ech0_score']:.1f}%")
            print(f"  Rank: {comp['rank']}/{len(comp['baselines']) + 1} among tested models")
            for model, baseline_score in comp['baselines'].items():
                diff = comp['ech0_score'] - baseline_score
                status = "ğŸŸ¢" if diff >= 0 else "ğŸ”´"
                print(f"    {status} vs {model}: {baseline_score:.1f}% ({diff:+.1f}%)")

    print("\nâœ… Benchmark testing complete!")
    print("ğŸ“Š Results saved to benchmark_results_*.json")


if __name__ == "__main__":
    asyncio.run(main())

